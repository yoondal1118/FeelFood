import os
import sys
import pickle
import numpy as np
import pandas as pd
import random
from tqdm import tqdm
import tensorflow as tf
from soynlp.normalizer import repeat_normalize
from transformers import AutoTokenizer, TFDistilBertModel

RAW_DATA_PATH = './raw/'
TRAIN_FILE = 'love_train.csv'
TEST_FILE = 'love_test.csv'
SAVE_PATH = './processed_data/'
MODEL_SAVE_PATH = 'love_model.h5'

MODEL_NAME = "monologg/distilkobert"
MAX_LEN = 256
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
EPOCHS = 5

if not os.path.exists(SAVE_PATH):
    os.makedirs(SAVE_PATH)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("GPU ì‚¬ìš© ì„¤ì • ì™„ë£Œ")
    except RuntimeError as e:
        print(e)

# ---------------------------------------------------------
# [ê°œì„  1] ë°ì´í„° ë…¸ì´ì¦ˆ ì¶”ê°€ í•¨ìˆ˜ (ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ)
# ---------------------------------------------------------
def add_noise(text, p_del=0.1, p_swap=0.1):
    """
    í…ìŠ¤íŠ¸ì— ì¸ìœ„ì ì¸ ë…¸ì´ì¦ˆ(ê¸€ì ì‚­ì œ, ìˆœì„œ ë³€ê²½)ë¥¼ ì¶”ê°€í•˜ì—¬
    ëª¨ë¸ì´ ì™„ë²½í•œ ë¬¸ì¥ íŒ¨í„´ë§Œ ì™¸ìš°ëŠ” ê²ƒì„ ë°©ì§€í•¨.
    """
    if not isinstance(text, str): return ""

    # 1. ë°˜ë³µ ë¬¸ì ì •ê·œí™” (ê¸°ì¡´)
    text = repeat_normalize(text, num_repeats=2)

    # í•™ìŠµ ë°ì´í„°ì—ë§Œ ë…¸ì´ì¦ˆ ì ìš© (Trainì—ì„œë§Œ í˜¸ì¶œí•  ê²ƒ)
    chars = list(text)
    n = len(chars)
    if n < 2: return text

    # ëœë¤ ì‚­ì œ
    if random.random() < p_del:
        idx = random.randint(0, n-1)
        del chars[idx]
        n -= 1

    # ëœë¤ êµí™˜ (ì˜¤íƒ€ ì‹œë®¬ë ˆì´ì…˜)
    if n > 1 and random.random() < p_swap:
        idx = random.randint(0, n-2)
        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]

    return "".join(chars)

def load_and_preprocess_data(filepath, is_train=False):
    if not os.path.exists(filepath):
        print(f"íŒŒì¼ ì—†ìŒ: {filepath}")
        return None

    # [ìˆ˜ì •] êµ¬ë¶„ì(sep)ë¥¼ ëª…í™•íˆ ì§€ì •í•˜ê³ , í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    data = pd.read_csv(filepath, sep='\t')

    # í˜¹ì‹œ ëª¨ë¥¼ ê²°ì¸¡ì¹˜ ì œê±°
    data = data.dropna(subset=['review', 'label'])

    # [ì¤‘ìš”] Train ë°ì´í„°ì—ë§Œ ë…¸ì´ì¦ˆë¥¼ ì„ì–´ì„œ í•™ìŠµ ë‚œì´ë„ë¥¼ ë†’ì„
    if is_train:
        tqdm.pandas(desc="í•™ìŠµ ë°ì´í„° ë…¸ì´ì¦ˆ ì£¼ì… ì¤‘")
        data['review'] = data['review'].progress_apply(lambda x: add_noise(x, p_del=0.15, p_swap=0.15))
    else:
        # Test ë°ì´í„°ëŠ” ì •ê·œí™”ë§Œ ìˆ˜í–‰
        data['review'] = data['review'].apply(lambda x: repeat_normalize(x, num_repeats=2))

    return data

# ë°ì´í„° ë¡œë“œ
print("[ì „ì²˜ë¦¬] ë°ì´í„° ë¡œë“œ ì¤‘...")
train_data = load_and_preprocess_data(os.path.join(RAW_DATA_PATH, TRAIN_FILE), is_train=True)
test_data = load_and_preprocess_data(os.path.join(RAW_DATA_PATH, TEST_FILE), is_train=False)

if train_data is None or test_data is None: sys.exit(1)

# í† í¬ë‚˜ì´ì§•
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

def bert_tokenize(texts, tokenizer, max_len):
    return tokenizer(
        texts.tolist(),
        truncation=True,
        padding='max_length',
        max_length=max_len,
        return_token_type_ids=False,
        return_tensors='tf'
    )

train_encodings = bert_tokenize(train_data['review'], tokenizer, MAX_LEN)
test_encodings = bert_tokenize(test_data['review'], tokenizer, MAX_LEN)

# ë°ì´í„°ì…‹ ìƒì„±
def create_tf_dataset(encodings, labels, batch_size, is_train=True):
    dataset = tf.data.Dataset.from_tensor_slices((
        {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']},
        labels
    ))
    if is_train:
        dataset = dataset.shuffle(20000, reshuffle_each_iteration=True)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

train_dataset = create_tf_dataset(train_encodings, train_data['label'].values, BATCH_SIZE, True)
test_dataset = create_tf_dataset(test_encodings, test_data['label'].values, BATCH_SIZE, False)

# ---------------------------------------------------------
# [ê°œì„  2] ëª¨ë¸ êµ¬ì¡° ë³€ê²½ (Layer Freezing & Dropout ì¦ê°€)
# ---------------------------------------------------------
class DistilBertLayer(tf.keras.layers.Layer):
    def __init__(self, model_name, **kwargs):
        super().__init__(**kwargs)
        self.bert = TFDistilBertModel.from_pretrained(model_name, from_pt=True)

    def call(self, inputs):
        return self.bert(inputs[0], attention_mask=inputs[1])[0]

def build_improved_model():
    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="input_ids")
    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="attention_mask")

    bert_layer = DistilBertLayer(MODEL_NAME)
    bert_layer.trainable = True

    last_hidden_state = bert_layer([input_ids, attention_mask])
    cls_token = last_hidden_state[:, 0, :]

    # 1. Dropout (ë¹„ìœ¨ì€ ìœ ì§€í•˜ë˜ ì¸µ ë‹¨ìˆœí™”)
    # Code 2ì—ì„œëŠ” 0.2ì˜€ìœ¼ë‚˜, Code 1ì˜ 0.3ì„ ìœ ì§€í•´ë„ í° ë¬¸ì œëŠ” ì—†ìŠµë‹ˆë‹¤.
    # ë‹¤ë§Œ êµ¬ì¡°ì  í†µì¼ì„±ì„ ìœ„í•´ Code 2ì™€ ë¹„ìŠ·í•˜ê²Œ ë§ì¶”ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    x = tf.keras.layers.Dropout(0.2)(cls_token)

    # 2. ì¤‘ê°„ Dense(64) ì¸µ ì œê±° (í•µì‹¬!)
    # ë°”ë¡œ ì¶œë ¥ì¸µìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)

    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)

    optimizer = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=0.01)
    loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)
    metrics = [
        'accuracy',
        tf.keras.metrics.AUC(name='auc', curve='PR')
    ]

    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)
    return model

model = build_improved_model()
model.summary()

# ---------------------------------------------------------
# í•™ìŠµ
# ---------------------------------------------------------
checkpoint_path = os.path.join(SAVE_PATH, MODEL_SAVE_PATH)

# EarlyStopping Patience ì¦ê°€ (ë…¸ì´ì¦ˆ ë•Œë¬¸ì— lossê°€ ì§„ë™í•  ìˆ˜ ìˆìŒ)
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)
mc = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)

print(f"[í•™ìŠµ] ì‹œì‘ (Epochs: {EPOCHS})...")
history = model.fit(
    train_dataset,
    epochs=EPOCHS,
    validation_data=test_dataset,
    callbacks=[es, mc]
)

# ---------------------------------------------------------
# ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
# ---------------------------------------------------------
print("\n[ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘]")
inference_model = build_improved_model()
inference_model.load_weights(checkpoint_path)

examples = [
    "ì—„ì²­ ì¹œì ˆí•˜ì‹œê³  ì¹˜í‚¨ë„ ë„ˆë¬´ ë¶€ë“œëŸ¬ìš´ë° ì–‘ë„ ë§ì•„ìš” ã… ìƒë§¥ì€ ì‹œì›í•˜êµ¬ ë„˜ ,, ì™„ë²½í•œ ë§›ì§‘",
    "ì£¼ì¸ì´ ì†ë‹˜ ê°€ë ¤ì„œ ëŒ€ì‘, ëˆ ë§ì´ ì•ˆì“°ë©´ ì¸ì‚¬ë„ ì•ˆí•¨. ì¹œì ˆí•œì²™ ì†ë‹˜ ê°€ë ¤ì„œ ëŒ€ì‘í•˜ëŠ”ê±° ì–´íœ´...",
    "ìŒì‹ ë§›ìˆê¸´ í•œë° ì¼ë‹¨ ë‚´ë¶€ê°€ ë„ˆë¬´ ë”ëŸ¬ì›Œìš” ì „ì— ì˜¬ ë•Œë„ ë§ì´ ëŠê¼ˆëŠ”ë° ê°ˆìˆ˜ë¡ ë” ë”ëŸ¬ì›Œì§€ë„¤ìš”",
    "ì˜¤ëŠ˜ ì‹œí—˜ ê°œë¹¡ì„¸ì„œ ë„ˆë¬´ í˜ë“¤ì—ˆëŠ”ë°, ìŒì‹ë¨¹ìœ¼ë‹ˆê¹Œ ì†ì´ ë»¥ ëš«ë ¤ì„œ ì¢‹ì•˜ì–´ìš”",
    "ë°°ë‹¬ì´ ëŠ¦ì–´ì„œ ë” ë¹¡ì³ìš”",
    "ë§›ì€ ìˆëŠ”ë° ì–‘ì´ ì¢€ ì ë„¤ìš”.",
    "ì—„ì²­ ì¹œì ˆí•˜ì‹œê³  ì¹˜í‚¨ë„ ë„ˆë¬´ ë¶€ë“œëŸ¬ìš´ë° ì–‘ë„ ë§ì•„ìš” ã… ìƒë§¥ì€ ì‹œì›í•˜êµ¬ ë„˜ ,, ì™„ë²½í•œ ë§›ì§‘ ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ì§„ì§œ ë„˜ ë§›ìˆì–´ì—¬.,,â¤ï¸â¤ï¸",
    "ì € ì´ë ‡ê²Œ ë§›ìˆëŠ” ì¹˜í‚¨ ì²˜ìŒ ë¨¹ì–´ë´ìš”â€¦ğŸ˜­ í˜¸ë°”íŠ¸ë‘ ê°„ì¥ ìˆœì‚´ ë°˜ë°˜ ì‹œì¼°ëŠ”ë° ë‘˜ë‹¤ë ˆì „ë“œì¡´ë§› ì–‘ë…ì´ ì—„ì²­ ì˜ ë²„ë¬´ëŸ¬ì ¸ìˆëŠ”ë° ì¹˜í‚¨ì´ ë°”ì‚­í•´ì—¬â€¦ğŸ‘¼ğŸ» í˜¸ë°”íŠ¸ëŠ” ê¼­ ë“œì„¸ìš”ã…  ì‹œì¤‘ì˜ ì²­ì–‘ë§ˆìš”ë“¤ì´ë‘ ë­”ê°€ ë‹¤ë¥¸ ë§›ì´ ë‚˜ëŠ”ë° ê·¸ê²Œ ë„˜ë„˜ ë§ˆì‹°ì–´ìš”ğŸ’—",
    "ìˆœì‚´ë¡œ ë°˜ë°˜ ë‘ ë§ˆë¦¬ ì‹œì¼°ì–´ìš”. ìˆœì‚´, ê°„ì¥, ì–‘ë…, ê°ˆë¦­ ì‹œì¼°ëŠ”ë° ì‚°ë”ë¯¸ë¡œ ë‚˜ì™”ë„¤ìš”. ",
    "ì£¼ì¸ì´ ì†ë‹˜ ê°€ë ¤ì„œ ëŒ€ì‘, ëˆ ë§ì´ ì•ˆì“°ë©´ ì¸ì‚¬ë„ ì•ˆí•¨. ì¹œì ˆí•œì²™ ì†ë‹˜ ê°€ë ¤ì„œ ëŒ€ì‘í•˜ëŠ”ê±° ì–´íœ´...",
    "ìŒì‹ ë§›ìˆê¸´ í•œë° ì¼ë‹¨ ë‚´ë¶€ê°€ ë„ˆë¬´ ë”ëŸ¬ì›Œìš” ì „ì— ì˜¬ ë•Œë„ ë§ì´ ëŠê¼ˆëŠ”ë° ê°ˆìˆ˜ë¡ ë” ë”ëŸ¬ì›Œì§€ë„¤ìš” ì¢…ì´ì»µì— ê³ ì¶§ê°€ë£¨ ë¬»ì–´ìˆê³  ë¬¼í†µì—ë„ ê³ ì¶§ê°€ë£¨ ë¶™ì–´ìˆê³  ë°¥ ê·¸ë¦‡ì—ë„ ë¶™ì–´ìˆê³ ;;",
    "ê·¸ë¦¬ê³  ì œê°€ ì—¬ê¸° ë§›ì„ ì•„ëŠ”ë° ì¼ë°˜ ì‹œì¼°ë”ë‹ˆ ì•ˆê²½ ì“´ ì•Œë°”ìƒì´ ì‚´ì§ ì§¸ë ¤ë³´ë”ë‹ˆ ë§¤ìš´ë§›ìœ¼ë¡œ ë°”ê¿”ì„œ ì£¼ë„¤ìš” ã…‹ã…‹ ê·¸ë˜ë†“ê³  ë§¤ìš´ë§›ìœ¼ë¡œ ë°”ê¿¨ëƒë‹ˆê¹Œ ë êº¼ìš´ í‘œì •ìœ¼ë¡œ 'ì•„ë‹ˆìš”' í•œë§ˆë”” í•˜ê³  ë§ˆëŠ”ë° ì„œë¹„ìŠ¤ê°€ ë„ˆë¬´ ë³„ë¡œì—¬ì„œ ë‹¤ì‹œëŠ” ì•ˆ ì˜¬ê±° ê°™ì•„ìš”~ ë¬´ìŠ¨ ì–‘ì•„ì¹˜ë“¤ì´ ì•Œë°”í•˜ëŠ”ì¤„ ì•Œì•˜ë„¤",
    "ì˜¤ëŠ˜ ê¸°ë¶„ ë‚˜ìœ ì¼ì´ ìˆì–´ì„œ ë‚¨ê¹ë‹ˆë‹¤.ì²˜ìŒ ë§¤ì¥ ì•ˆì— ë“¤ì–´ì™”ì„ë•Œ 4ì¸í…Œì´ë¸”ì— 2ëª…ì”© ì•‰ì•„ìˆëŠ” 2íŒ€ì˜ ì†ë‹˜ì´ ìˆì—ˆìŠµë‹ˆë‹¤.ê·¸ë˜ì„œ ì €í¬ë„ 2ëª…ì´ì§€ë§Œ ë§ì€ ì†ë‹˜ì´ ì—†ì–´ì„œ 4ì¸í…Œì´ë¸”ì— ì•‰ì•˜ì–´ìš”. ê·¸ë¬ë”ë‹ˆ 2ì¸í…Œì´ë¸”ë¡œ ê°€ë¼ê³  í•˜ì‹œë”ë¼ê³ ìš”? ì•ìœ¼ë¡œ ì†ë‹˜ ë” ë§ì•„ì§ˆêº¼ê°™ìœ¼ë‹ˆ ê·¸ëŸ°ê°€ë³´ë‹¤~ í–ˆëŠ”ë° ì‹ì‚¬ í•˜ê³  ìˆëŠ”ë° ì €í¬ ë’¤ë¡œ ì˜¨ ì†ë‹˜ë“¤ë„ 2ëª…ì¸ë° 4ì¸ í…Œì´ë¸”ì— ì•‰ì•„ë„ ì•„ë¬´ ë§ë„ ì•ˆí•˜ì‹œë”ë¼ê³ ìš”ë§¤ì¥ì•ˆì— ìˆëŠ” ëª¨ë“  ì†ë‹˜ì´ ë‹¤ 2ëª…ì”© ì™”ëŠ”ë° ì €í¬í•œí…Œë§Œ ê·¸ëŸ¬ì‹œë‹ˆ ê¸°ë¶„ì´ ë‚˜ì˜ë”ë¼ê³ ìš”?ëª¨~ë‘ ê³µí‰í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”~ ã…‹ã…‹",
    "ì €ëŠ” ë§¤ìš´ ìŒì‹ì„ ì •ë§ ì¢‹ì•„í•˜ê³ , ì œ ì£¼ë³€ì—ëŠ” ë§¤ìš´ ìŒì‹ì„ ì €ë§Œí¼ ì˜ ë¨¹ëŠ” ì‚¬ëŒì´ ì—†ìŠµë‹ˆë‹¤. ì‹ ê¸¸ë™ ì§¬ë½• 2ë²ˆ ì™„ë½• ê²½í—˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ ë§µë¶€ì‹¬ ë¿œë¿œí•œ ìƒíƒœë¡œ, 10ì—¬ ë…„ê°„ ê¿ˆì—ì„œë§Œ ë³´ì•˜ë˜ ë””ì§„ë‹¤ ëˆê¹ŒìŠ¤ë¥¼ ë¨¹ìœ¼ëŸ¬ ì™”ì–´ìš”. ì½”ë¥¼ ì°Œë¥´ëŠ”, ì²˜ìŒ ë§¡ì•„ë³´ëŠ” ë§¤ìš´ ëƒ„ìƒˆì— ê²ì„ ë¨¹ì—ˆë‹¤ê°€ ì²« ì…ì„ ë¨¹ì€ ìˆœê°„ ë„ˆë¬´ ëœ¨ê±°ì›Œì„œ ì…ì²œì¥ì´ ë°”ë¡œ ë²—ê²¨ì¡Œì–´ìš” ..ğŸ˜‚ íŠ€ê¹€ì˜· ë¶„ë¦¬ ì´ìŠˆë§Œ ì•„ë‹ˆë©´ ë‹¤ ì¢‹ì•˜ì„í…ë° ê·¸ê²ƒ ë§ê³ ëŠ” ë­, ê³ ê¸° ì¡ë‚´ë„ ì—†ê³ , ë°”ì‚­í•˜ê³ , ìƒê°ë³´ë‹¤ ê¸°ë¶„ ì¢‹ê²Œ ë§¤ìš´ë§›ì´ë¼ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ê³µë³µì— ê²”í¬ìŠ¤ í•˜ë‚˜ ë¨¹ê³  ë¨¹ì€ê±´ë°ë„ ì†ì´ ì‹ ê¸°í•˜ê²Œ ê´œì°®ì•„ìš”. ê°œì¸ì ìœ¼ë¡œ 'ëˆê¹ŒìŠ¤' ëŠ” ì œê°€ êµ³ì´ ëˆ ì£¼ê³  ì‚¬ ë¨¹ëŠ” ìŒì‹ì€ ì•„ë‹™ë‹ˆë‹¤ë§Œ ë””ì§„ë‹¤ ì†ŒìŠ¤ ë•Œë¬¸ì— ì—¬ê¸°ê°€ ë˜ ìƒê°ë‚  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë°°ë¶ˆëŸ¬ì„œ ë‚¨ê²¼ëŠ”ë°, í¬ì¥ì´ ë¶ˆê°€í•œ ì ì€ ë„ˆë¬´ ì•„ì‰¬ì›Œìš”.",
    "ì˜¤ëŠ˜ ì‹œí—˜ ê°œë¹¡ì„¸ì„œ ë„ˆë¬´ í˜ë“¤ì—ˆëŠ”ë°, ìŒì‹ë¨¹ìœ¼ë‹ˆê¹Œ ì†ì´ ë»¥ ëš«ë ¤ì„œ ì¢‹ì•˜ì–´ìš”",
    "ë„ˆë¬´ í˜ë“ ì¼ì´ ìˆì—ˆëŠ”ë°, ì„œë¹„ìŠ¤ê°€ ì œ ë§ˆìŒì„ ë…¹ì˜€ì–´ìš”",
    "ì§„ì§œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•˜ëŠ”ë°, ë°°ë‹¬ì´ ëŠ¦ì–´ì„œ ë” ë¹¡ì³ìš”"
]

for text in examples:
    # ì¶”ë¡  ì‹œì—ëŠ” ë…¸ì´ì¦ˆ ì—†ì´ clean_textë§Œ
    cleaned = repeat_normalize(text, num_repeats=2)
    encodings = tokenizer([cleaned], truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='tf')
    pred = inference_model.predict({'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']}, verbose=0)[0][0]

    label = "ì‚¬ë‘(Love)" if pred > 0.5 else "ê·¸ì™¸(Other)"
    print(f"ë¬¸ì¥: {text}\n -> ì˜ˆì¸¡: {label} ({pred*100:.2f}%)")
    print("-" * 30)