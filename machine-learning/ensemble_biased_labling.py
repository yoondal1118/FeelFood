import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from soynlp.normalizer import repeat_normalize
from transformers import AutoTokenizer, TFDistilBertModel
from tqdm import tqdm

# ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ê²°ê³¼ê°€ ë˜‘ê°™ì´ ë‚˜ì˜¤ë„ë¡ í•´ì‰¬ê°’ì„ ê³ ì •í•©ë‹ˆë‹¤.
def reset_seeds(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    tf.random.set_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

reset_seeds() # í•¨ìˆ˜ ì‹¤í–‰

MAX_LEN = 512
MODEL_NAME = "monologg/distilkobert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# ì‹¤ì œ ë¦¬ë·°ê°€ ìˆëŠ” í´ë”
PATH = "./12_16_good_result/"

# ê°ì •ë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ
EMOTION_FILES = {
    'í¬(Happy)':   f'{PATH}happy_model.h5',
    'ë…¸(Angry)':   f'{PATH}angry_model.h5',
    'ì• (Sad)':     f'{PATH}sad_model.h5',
    'ì• (Love)':    f'{PATH}love_model.h5',
    'ë½(Fun)':     f'{PATH}fun_model.h5',
    'ë¶ˆë§Œ(Complaint)': f'{PATH}complaint_model.h5'
}

class DistilBertLayer(tf.keras.layers.Layer):
    def __init__(self, model_name, **kwargs):
        super().__init__(**kwargs)
        self.bert = TFDistilBertModel.from_pretrained(model_name, from_pt=True)

    def call(self, inputs):
        input_ids = inputs[0]
        attention_mask = inputs[1]
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return outputs[0]

def clean_text(text):
    if not isinstance(text, str): return ""
    return repeat_normalize(text, num_repeats=2)

def build_model():
    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="input_ids")
    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="attention_mask")

    bert_layer = DistilBertLayer(MODEL_NAME)
    last_hidden_state = bert_layer([input_ids, attention_mask])
    cls_token = last_hidden_state[:, 0, :]

    x = tf.keras.layers.Dropout(0.2)(cls_token)

    # ì´ë¦„ì„ ê°•ì œë¡œ ì§€ì •í•´ì¤ë‹ˆë‹¤ (ì €ì¥ëœ íŒŒì¼ê³¼ ë§ì¶”ê¸° ìœ„í•¨)
    output = tf.keras.layers.Dense(1, activation='sigmoid', name='dense')(x)

    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# 2. ëª¨ë“  ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_all_emotion_models(emotion_files):
    loaded_models = {}
    print(f"[ì‹œìŠ¤í…œ] ì´ {len(emotion_files)}ê°œì˜ ê°ì • ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

    for emotion_name, file_path in emotion_files.items():
        if not os.path.exists(file_path):
            print(f"  [ê²½ê³ ] íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            continue

        try:
            # ë©”ëª¨ë¦¬ ì²­ì†Œ! ì´ê±¸ í•´ì•¼ ì´ë¦„ì´ dense_1, dense_2ë¡œ ì•ˆ ë°”ë€ë‹ˆë‹¤.
            tf.keras.backend.clear_session()

            model = build_model()
            model.load_weights(file_path, by_name=True)
            loaded_models[emotion_name] = model
            print(f"  - ë¡œë“œ ì„±ê³µ: {emotion_name}")

        except Exception as e:
            print(f"  [ì¹˜ëª…ì  ì˜¤ë¥˜] {emotion_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    if not loaded_models:
        print("\n[ë¹„ìƒ] ë¡œë“œëœ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤!")
        exit()

    return loaded_models

my_models = load_all_emotion_models(EMOTION_FILES)

# 3. í†µí•© ì˜ˆì¸¡ í•¨ìˆ˜
def predict_multi_emotion(text, models, tokenizer):
    cleaned_text = clean_text(text)
    encodings = tokenizer(cleaned_text, truncation=True, padding='max_length', max_length=MAX_LEN, return_token_type_ids=False, return_tensors='tf')
    inputs = {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']}

    # í¼ì„¼íŠ¸ ì¡°ì ˆ
    BIAS_SCORES = {
        'í¬(Happy)': -0.05,
        'ë…¸(Angry)': 0.1,
        'ì• (Sad)': -0.05,
        'ì• (Love)': 0.03,
        'ë½(Fun)': 0.0,
        'ë¶ˆë§Œ(Complaint)': 0.2
    }

    scores = {}
    # print(f"\n[ìƒì„¸ ì ìˆ˜í‘œ] ë¬¸ì¥: {text[:20]}...")
    for emotion_name, model in models.items():
        try:
            pred = model(inputs, training=False)
            raw_prob = float(pred[0][0])

            bias = BIAS_SCORES.get(emotion_name, 0.0)
            final_prob = max(0.0, min(1.0, raw_prob + bias))

            scores[emotion_name] = round(final_prob, 4)
            # bar = "â– " * int(final_prob * 10)
            # print(f"  - {emotion_name}: {final_prob:.4f} (ì›ì ìˆ˜: {raw_prob:.4f}) {bar}")

        except Exception as e:
            scores[emotion_name] = 0.0

    if not scores: return "ì—ëŸ¬", 0.0, {}
    best_emotion = max(scores, key=scores.get)
    best_score = scores[best_emotion]
    return best_emotion, best_score, scores

# [í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° CSV ì €ì¥]
print("\n[ë‹¤ì¤‘ ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸]")
FILEPATH = './12_16_good_result/review.csv'
data = pd.read_csv(FILEPATH)
data = data.dropna(subset=['r_content'])
label = []
THRESHOLD = 0.5
results_list = []

#for text in tqdm(data['r_content'][0:50]):
for text in tqdm(data['r_content']):
    predicted_label, confidence, all_scores = predict_multi_emotion(text, my_models, tokenizer)
    final_label = predicted_label
    if confidence < THRESHOLD:
        final_label = "ë¬´ê°ì •/ëª¨ë¦„"
    if final_label == 'í¬(Happy)':
        label.append(1)
    elif final_label == 'ë…¸(Angry)':
        label.append(2)
    elif final_label == 'ì• (Sad)':
        label.append(3)
    elif final_label == 'ì• (Love)':
        label.append(4)
    elif final_label == 'ë½(Fun)':
        label.append(5)
    elif final_label == 'ë¶ˆë§Œ(Complaint)':
        label.append(9)
    print(f"ë¬¸ì¥: {text[:50]}...")
    print(f"ğŸ‘‰ ê²°ê³¼: {final_label} ({confidence*100:.2f}%)")
    print("-" * 50)

    row_data = {'ë¦¬ë·°ë‚´ìš©': text, 'ìµœì¢…ì˜ˆì¸¡': final_label, 'í™•ì‹ ë„': f"{confidence:.2f}", '1ìˆœìœ„ê°ì •': predicted_label}
    row_data.update(all_scores)
    results_list.append(row_data)

data['r_label'] = pd.Series(label, dtype='Int64')
data.to_csv('./12_16_good_result/result_data.csv', index=False, encoding='utf-8-sig', na_rep='')
